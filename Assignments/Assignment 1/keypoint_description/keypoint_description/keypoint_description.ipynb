{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoint(Patch) Description\n",
    "  \n",
    "This project will be all about defining and training a convolutional neural network to perform keypoint  description. \n",
    "PyTorch tutorials are available at: https://github.com/yunjey/pytorch-tutorial\n",
    "\n",
    "\n",
    "The first step is to load and visualize the data you'll be working with. \n",
    "\n",
    "We will use below dataset in this project:\n",
    "###  The Photo Tourism dataset \n",
    "(http://phototour.cs.washington.edu/patches/default.htm)\n",
    "\n",
    "It is also available in PyTorch torchvision datasets:\n",
    "https://pytorch.org/docs/stable/_modules/torchvision/datasets/phototour.html#PhotoTour\n",
    "\n",
    "This dataset consists of 1024 x 1024 bitmap (.bmp) images, each containing a 16 x 16 array of image patches. Here are some examples:\n",
    "\n",
    "<table><tr><td><img src='images/patches0001.bmp'></td><td><img src='images/patches1482.bmp'></td></tr></table>    \n",
    "For details of how the scale and orientation is established, please see the paper:  \n",
    "<p class=\"style8\"><font size=\"2\">S. Winder and M. Brown. <strong>Learning Local Image \n",
    "\t\t\t\tDescriptors</strong>. To appear <i>International Conference on \n",
    "\t\t\t\tComputer Vision and Pattern Recognition (CVPR2007)</i> (</font><a href=\"http://research.microsoft.com/~swinder/papers/winder_brown_cvpr07.pdf\"><span class=\"style9\">pdf \n",
    "\t\t\t\t300Kb</span></a><font size=\"2\">)</font></p>\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 102261.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy, copy\n",
    "from config_profile import args\n",
    "from Utils import cv2_scale36, cv2_scale, np_reshape, np_reshape64\n",
    "print(\"Done!\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "print(\"Done!\")\n",
    "class TripletPhotoTour(dset.PhotoTour):\n",
    "    \"\"\"\n",
    "    From the PhotoTour Dataset it generates triplet samples\n",
    "    note: a triplet is composed by a pair of matching images and one of\n",
    "    different class.\n",
    "    \"\"\"\n",
    "    def __init__(self, train=True, transform=None, batch_size = None,load_random_triplets = False,  *arg, **kw):\n",
    "        super(TripletPhotoTour, self).__init__(*arg, **kw)\n",
    "        self.transform = transform\n",
    "        self.out_triplets = load_random_triplets\n",
    "        self.train = train\n",
    "        self.n_triplets = args.n_triplets\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if self.train:\n",
    "            print('Generating {} triplets'.format(self.n_triplets))\n",
    "            self.triplets = self.generate_triplets(self.labels, self.n_triplets)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_triplets(labels, num_triplets):\n",
    "        def create_indices(_labels):\n",
    "            inds = dict()\n",
    "            for idx, ind in enumerate(_labels):\n",
    "                if ind not in inds:\n",
    "                    inds[ind] = []\n",
    "                inds[ind].append(idx)\n",
    "            return inds\n",
    "\n",
    "        triplets = []\n",
    "        indices = create_indices(labels.numpy())\n",
    "        unique_labels = np.unique(labels.numpy())\n",
    "        n_classes = unique_labels.shape[0]\n",
    "        # add only unique indices in batch\n",
    "        already_idxs = set()\n",
    "\n",
    "        for x in tqdm(range(num_triplets)):\n",
    "            if len(already_idxs) >= args.batch_size:\n",
    "                already_idxs = set()\n",
    "            c1 = np.random.randint(0, n_classes)\n",
    "            while c1 in already_idxs:\n",
    "                c1 = np.random.randint(0, n_classes)\n",
    "            already_idxs.add(c1)\n",
    "            c2 = np.random.randint(0, n_classes)\n",
    "            while c1 == c2:\n",
    "                c2 = np.random.randint(0, n_classes)\n",
    "            if len(indices[c1]) == 2:  # hack to speed up process\n",
    "                n1, n2 = 0, 1\n",
    "            else:\n",
    "                n1 = np.random.randint(0, len(indices[c1]))\n",
    "                n2 = np.random.randint(0, len(indices[c1]))\n",
    "                while n1 == n2:\n",
    "                    n2 = np.random.randint(0, len(indices[c1]))\n",
    "            n3 = np.random.randint(0, len(indices[c2]))\n",
    "            triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n",
    "        return torch.LongTensor(np.array(triplets))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        def transform_img(img):\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img.numpy())\n",
    "            return img\n",
    "\n",
    "        if not self.train:\n",
    "            m = self.matches[index]\n",
    "            img1 = transform_img(self.data[m[0]])\n",
    "            img2 = transform_img(self.data[m[1]])\n",
    "            return img1, img2, m[2]\n",
    "\n",
    "        t = self.triplets[index]\n",
    "        a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n",
    "\n",
    "        img_a = transform_img(a)\n",
    "        img_p = transform_img(p)\n",
    "        img_n = None\n",
    "        if self.out_triplets:\n",
    "            img_n = transform_img(n)\n",
    "        # transform images if required\n",
    "        if args.fliprot:\n",
    "            do_flip = random.random() > 0.5\n",
    "            do_rot = random.random() > 0.5\n",
    "            if do_rot:\n",
    "                img_a = img_a.permute(0,2,1)\n",
    "                img_p = img_p.permute(0,2,1)\n",
    "                if self.out_triplets:\n",
    "                    img_n = img_n.permute(0,2,1)\n",
    "            if do_flip:\n",
    "                img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n",
    "                img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n",
    "                if self.out_triplets:\n",
    "                    img_n = torch.from_numpy(deepcopy(img_n.numpy()[:,:,::-1]))\n",
    "        if self.out_triplets:\n",
    "            return (img_a, img_p, img_n)\n",
    "        else:\n",
    "            return (img_a, img_p)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return self.triplets.size(0)\n",
    "        else:\n",
    "            return self.matches.size(0)\n",
    "print(\"Done!\")\n",
    "def create_loaders(dataset_names, load_random_triplets = False):\n",
    "    test_dataset_names = copy(dataset_names)\n",
    "    test_dataset_names.remove(args.training_set)\n",
    "\n",
    "    kwargs = {'num_workers': args.num_workers, 'pin_memory': args.pin_memory} if args.cuda else {}\n",
    "\n",
    "    #np_reshape64 = lambda x: np.reshape(x, (16, 16, 1))\n",
    "    np_reshape64 = lambda x: np.reshape(x, (64, 64, 1))\n",
    "    transform_test = transforms.Compose([\n",
    "            transforms.Lambda(np_reshape64),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor()])\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.Lambda(np_reshape64),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomRotation(5,PIL.Image.BILINEAR),\n",
    "            #transforms.RandomResizedCrop(8, scale = (0.9,1.0),ratio = (0.9,1.1)),\n",
    "            #transforms.Resize(8),\n",
    "            transforms.RandomResizedCrop(32, scale = (0.9,1.0),ratio = (0.9,1.1)),\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor()])\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Lambda(cv2_scale),\n",
    "            transforms.Lambda(np_reshape),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((args.mean_image,), (args.std_image,))])\n",
    "    if not args.augmentation:\n",
    "        transform_train = transform\n",
    "        transform_test = transform\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            TripletPhotoTour(train=True,\n",
    "                             load_random_triplets = load_random_triplets,\n",
    "                             batch_size=args.batch_size,\n",
    "                             root=args.dataroot,\n",
    "                             name=args.training_set,\n",
    "                             download=True,\n",
    "                             transform=transform_train),\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=False, **kwargs)\n",
    "\n",
    "    test_loaders = [{'name': name,\n",
    "                     'dataloader': torch.utils.data.DataLoader(\n",
    "             TripletPhotoTour(train=False,\n",
    "                     batch_size=args.test_batch_size,\n",
    "                     root=args.dataroot,\n",
    "                     name=name,\n",
    "                     download=True,\n",
    "                     transform=transform_test),\n",
    "                        batch_size=args.test_batch_size,\n",
    "                        shuffle=False, **kwargs)}\n",
    "                    for name in test_dataset_names]\n",
    "\n",
    "    return train_loader, test_loaders\n",
    "print(\"Done!\")\n",
    "dataset_names = ['liberty', 'notredame', 'yosemite']\n",
    "train_loader, test_loaders = create_loaders(dataset_names, load_random_triplets = args.load_random_triplets)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Build Network Model\n",
    "The DesNet is a simple CNN network, which only contains two CNN blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Done!\n",
      "Done!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# load network\n",
    "CNN = 3\n",
    "model = None\n",
    "if CNN == 1:\n",
    "    from descriptor import DesNet_1\n",
    "    model = DesNet_1()\n",
    "elif CNN == 2:\n",
    "    from descriptor import DesNet_2\n",
    "    model = DesNet_2()\n",
    "elif CNN == 3:\n",
    "    from descriptor import DesNet_3\n",
    "    model = DesNet_3()\n",
    "elif CNN == 4:\n",
    "    from descriptor import DesNet_4\n",
    "    model = DesNet_4()\n",
    "else:\n",
    "    print(\"No such model exists!\")\n",
    "    exit(1)\n",
    "if model == None:\n",
    "    print(\"No model found!\")\n",
    "    exit(1)\n",
    "if args.cuda:\n",
    "        model.cuda()\n",
    "print(\"Done!\")\n",
    "# define optimizer\n",
    "def create_optimizer(model, new_lr):\n",
    "    # setup optimizer\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n",
    "                              momentum=0.9, dampening=0.9,\n",
    "                              weight_decay=args.wd)\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n",
    "                               weight_decay=args.wd)\n",
    "    else:\n",
    "        raise Exception('Not supported optimizer: {0}'.format(args.optimizer))\n",
    "    return optimizer\n",
    "optimizer1 = create_optimizer(model.features, args.lr)\n",
    "print(\"Done!\")\n",
    "def train(train_loader, model, optimizer, epoch, logger, load_triplets  = False, name = \"\"):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(train_loader))\n",
    "    for batch_idx, data in pbar:\n",
    "        if load_triplets:\n",
    "            data_a, data_p, data_n = data\n",
    "        else:\n",
    "            data_a, data_p = data\n",
    "\n",
    "        if args.cuda:\n",
    "            data_a, data_p  = data_a.cuda(), data_p.cuda()\n",
    "            data_a, data_p = Variable(data_a), Variable(data_p)\n",
    "            out_a = model(data_a)\n",
    "            out_p = model(data_p)\n",
    "        if load_triplets:\n",
    "            data_n  = data_n.cuda()\n",
    "            data_n = Variable(data_n)\n",
    "            out_n = model(data_n)\n",
    "        \n",
    "        \n",
    "        loss = loss_DesNet(out_a, out_p,\n",
    "                        margin=args.margin,\n",
    "                        anchor_swap=args.anchorswap,\n",
    "                        anchor_ave=args.anchorave,\n",
    "                        batch_reduce = args.batch_reduce,\n",
    "                        loss_type = args.loss)\n",
    "\n",
    "        if args.decor:\n",
    "            loss += CorrelationPenaltyLoss()(out_a)\n",
    "            \n",
    "        if args.gor:\n",
    "            loss += args.alpha*global_orthogonal_regularization(out_a, out_n)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        adjust_learning_rate(optimizer)\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(\n",
    "                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data_a), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader),\n",
    "                    loss.item()))\n",
    "    out_string = \"[\" + str(epoch) + \", \" + str(loss.item())\n",
    "    #print(out_string)\n",
    "    file = open('results-'+str(CNN)+'.txt', 'a')\n",
    "    file.write(out_string)\n",
    "    file.close()\n",
    "\n",
    "    if (args.enable_logging):\n",
    "#         logger.log_value('loss', loss.data[0]).step()\n",
    "        logger.log_value('loss', loss.item()).step()\n",
    "\n",
    "    try:\n",
    "        os.stat('{}{}'.format(args.model_dir,suffix))\n",
    "    except:\n",
    "        os.makedirs('{}{}'.format(args.model_dir,suffix))\n",
    "\n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict()},\n",
    "               '{}{}/checkpoint_{}.pth'.format(args.model_dir,suffix,epoch))\n",
    "    \n",
    "    \n",
    "def adjust_learning_rate(optimizer):\n",
    "    \"\"\"Updates the learning rate given the learning rate decay.\n",
    "    The routine has been implemented according to the original Lua SGD optimizer\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        if 'step' not in group:\n",
    "            group['step'] = 0.\n",
    "        else:\n",
    "            group['step'] += 1.\n",
    "        group['lr'] = args.lr * (\n",
    "        1.0 - float(group['step']) * float(args.batch_size) / (args.n_triplets * float(args.epochs)))\n",
    "    return\n",
    "print(\"Done!\")\n",
    "def test(test_loader, model, epoch, logger, logger_test_name):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    labels, distances = [], []\n",
    "\n",
    "    pbar = tqdm(enumerate(test_loader))\n",
    "    for batch_idx, (data_a, data_p, label) in pbar:\n",
    "        # data_a.shape= torch.Size([1024, 1, 32, 32]) \n",
    "        # data_p.shape =torch.Size([1024, 1, 32, 32]) \n",
    "        # label.shape = torch.Size([1024])\n",
    "        if args.cuda:\n",
    "            data_a, data_p = data_a.cuda(), data_p.cuda()\n",
    "\n",
    "        data_a, data_p, label = Variable(data_a, volatile=True), \\\n",
    "                                Variable(data_p, volatile=True), Variable(label)\n",
    "        out_a = model(data_a)\n",
    "        out_p = model(data_p)\n",
    "        dists = torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "        distances.append(dists.data.cpu().numpy().reshape(-1,1))\n",
    "        ll = label.data.cpu().numpy().reshape(-1, 1)\n",
    "        labels.append(ll)\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            pbar.set_description(logger_test_name+' Test Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
    "                epoch, batch_idx * len(data_a), len(test_loader.dataset),\n",
    "                       100. * batch_idx / len(test_loader)))\n",
    "\n",
    "    num_tests = test_loader.dataset.matches.size(0)\n",
    "    labels = np.vstack(labels).reshape(num_tests)\n",
    "    distances = np.vstack (distances).reshape(num_tests)\n",
    "\n",
    "    fpr95 = ErrorRateAt95Recall(labels, 1.0 / (distances + 1e-8))\n",
    "    print('\\33[91mTest set: Accuracy(FPR95): {:.8f}\\n\\33[0m'.format(fpr95))\n",
    "    out_string = \", \" + str(fpr95)\n",
    "    if logger_test_name == \"notredame\":\n",
    "        out_string += \", \"\n",
    "    else:\n",
    "        out_string += \"],\\n\"\n",
    "    #print(out_string)\n",
    "    file = open('results-'+str(CNN)+'.txt', 'a')\n",
    "    file.write(out_string)\n",
    "    file.close()\n",
    "\n",
    "    if (args.enable_logging):\n",
    "        logger.log_value(logger_test_name+' fpr95', fpr95)\n",
    "    return\n",
    "\n",
    "\n",
    "def ErrorRateAt95Recall(labels, scores):\n",
    "    distances = 1.0 / (scores + 1e-8)\n",
    "    recall_point = 0.95\n",
    "    labels = labels[np.argsort(distances)]\n",
    "    # Sliding threshold: get first index where recall >= recall_point. \n",
    "    # This is the index where the number of elements with label==1 below the threshold reaches a fraction of \n",
    "    # 'recall_point' of the total number of elements with label==1. \n",
    "    # (np.argmax returns the first occurrence of a '1' in a bool array). \n",
    "    threshold_index = np.argmax(np.cumsum(labels) >= recall_point * np.sum(labels)) \n",
    "\n",
    "    FP = np.sum(labels[:threshold_index] == 0) # Below threshold (i.e., labelled positive), but should be negative\n",
    "    TN = np.sum(labels[threshold_index:] == 0) # Above threshold (i.e., labelled negative), and should be negative\n",
    "    return float(FP) / float(FP + TN)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [40960/50000 (82%)]\tLoss: 0.929034: : 49it [00:17,  2.93it/s]\n",
      "0it [00:00, ?it/s]/scratch/saeedib/conda/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:130: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/scratch/saeedib/conda/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:131: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "notredame Test Epoch: 0 [92160/100000 (92%)]: : 98it [00:19,  5.71it/s]\n",
      "yosemite Test Epoch: 0 [0/100000 (0%)]: : 1it [00:00,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.21708000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 0 [92160/100000 (92%)]: : 98it [00:20,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.25834000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 100763.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 0.895880: : 49it [00:17,  2.97it/s]\n",
      "notredame Test Epoch: 1 [92160/100000 (92%)]: : 98it [00:19,  5.77it/s]\n",
      "yosemite Test Epoch: 1 [0/100000 (0%)]: : 1it [00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.16198000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 1 [92160/100000 (92%)]: : 98it [00:21,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.20700000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 85866.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 0.863435: : 49it [00:17,  2.90it/s]\n",
      "notredame Test Epoch: 2 [92160/100000 (92%)]: : 98it [00:19,  5.60it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[91mTest set: Accuracy(FPR95): 0.12298000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 2 [92160/100000 (92%)]: : 98it [00:19,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.16974000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 107654.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 0.823979: : 49it [00:17,  2.93it/s]\n",
      "notredame Test Epoch: 3 [92160/100000 (92%)]: : 98it [00:19,  5.54it/s]\n",
      "yosemite Test Epoch: 3 [0/100000 (0%)]: : 1it [00:00,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/sets/yosemite.pt\n",
      "\u001b[91mTest set: Accuracy(FPR95): 0.08592000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 3 [92160/100000 (92%)]: : 98it [00:19,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.13514000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 107454.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [40960/50000 (82%)]\tLoss: 0.810126: : 49it [00:17,  2.97it/s]\n",
      "notredame Test Epoch: 4 [92160/100000 (92%)]: : 98it [00:18,  5.85it/s]\n",
      "yosemite Test Epoch: 4 [0/100000 (0%)]: : 1it [00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/sets/yosemite.pt\n",
      "\u001b[91mTest set: Accuracy(FPR95): 0.06264000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 4 [92160/100000 (92%)]: : 98it [00:19,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.11552000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 107278.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 0.806462: : 49it [00:17,  2.96it/s]\n",
      "notredame Test Epoch: 5 [92160/100000 (92%)]: : 98it [00:19,  5.78it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.06808000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 5 [92160/100000 (92%)]: : 98it [00:19,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.11858000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 101900.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [40960/50000 (82%)]\tLoss: 0.799573: : 49it [00:17,  2.99it/s]\n",
      "notredame Test Epoch: 6 [92160/100000 (92%)]: : 98it [00:22,  4.51it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.05276000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 6 [92160/100000 (92%)]: : 98it [00:20,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.10460000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 100402.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [40960/50000 (82%)]\tLoss: 0.796307: : 49it [00:17,  2.97it/s]\n",
      "notredame Test Epoch: 7 [92160/100000 (92%)]: : 98it [00:19,  5.55it/s]\n",
      "yosemite Test Epoch: 7 [0/100000 (0%)]: : 1it [00:00,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.04626000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 7 [92160/100000 (92%)]: : 98it [00:19,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.09262000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 119474.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [40960/50000 (82%)]\tLoss: 0.784153: : 49it [00:17,  2.93it/s]\n",
      "notredame Test Epoch: 8 [92160/100000 (92%)]: : 98it [00:19,  5.73it/s]\n",
      "yosemite Test Epoch: 8 [0/100000 (0%)]: : 1it [00:00,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[91mTest set: Accuracy(FPR95): 0.04492000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 8 [92160/100000 (92%)]: : 98it [00:20,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.08894000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 104791.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [40960/50000 (82%)]\tLoss: 0.796317: : 49it [00:17,  2.81it/s]\n",
      "notredame Test Epoch: 9 [92160/100000 (92%)]: : 98it [00:19,  5.64it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.04374000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 9 [92160/100000 (92%)]: : 98it [00:19,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.09588000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n",
      "Generating 50000 triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 96022.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [40960/50000 (82%)]\tLoss: 0.777624: : 49it [00:17,  3.02it/s]\n",
      "notredame Test Epoch: 10 [92160/100000 (92%)]: : 98it [00:19,  5.78it/s]\n",
      "yosemite Test Epoch: 10 [0/100000 (0%)]: : 1it [00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.03886000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 10 [92160/100000 (92%)]: : 98it [00:19,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.09386000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 93216.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [40960/50000 (82%)]\tLoss: 0.763093: : 49it [00:17,  2.74it/s]\n",
      "notredame Test Epoch: 11 [92160/100000 (92%)]: : 98it [00:19,  5.55it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.03522000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 11 [92160/100000 (92%)]: : 98it [00:19,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.08296000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n",
      "Generating 50000 triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 103906.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [40960/50000 (82%)]\tLoss: 0.770307: : 49it [00:17,  2.93it/s]\n",
      "notredame Test Epoch: 12 [92160/100000 (92%)]: : 98it [00:19,  5.61it/s]\n",
      "yosemite Test Epoch: 12 [0/100000 (0%)]: : 1it [00:00,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.03414000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 12 [92160/100000 (92%)]: : 98it [00:19,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.08378000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 90428.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [40960/50000 (82%)]\tLoss: 0.768486: : 49it [00:17,  2.93it/s]\n",
      "notredame Test Epoch: 13 [92160/100000 (92%)]: : 98it [00:19,  5.48it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/sets/yosemite.pt\n",
      "\u001b[91mTest set: Accuracy(FPR95): 0.03562000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 13 [92160/100000 (92%)]: : 98it [00:19,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.08870000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 107732.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [40960/50000 (82%)]\tLoss: 0.756350: : 49it [00:17,  2.88it/s]\n",
      "notredame Test Epoch: 14 [92160/100000 (92%)]: : 98it [00:19,  5.77it/s]\n",
      "yosemite Test Epoch: 14 [0/100000 (0%)]: : 1it [00:00,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.03368000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 14 [92160/100000 (92%)]: : 98it [00:19,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.08644000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 112043.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [40960/50000 (82%)]\tLoss: 0.752152: : 49it [00:18,  2.78it/s]\n",
      "notredame Test Epoch: 15 [92160/100000 (92%)]: : 98it [00:23,  4.62it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/sets/yosemite.pt\n",
      "\u001b[91mTest set: Accuracy(FPR95): 0.03136000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 15 [92160/100000 (92%)]: : 98it [00:22,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.07750000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 89552.11it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [40960/50000 (82%)]\tLoss: 0.774919: : 49it [00:17,  2.89it/s]\n",
      "notredame Test Epoch: 16 [92160/100000 (92%)]: : 98it [00:19,  5.77it/s]\n",
      "yosemite Test Epoch: 16 [0/100000 (0%)]: : 1it [00:00,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.03140000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 16 [92160/100000 (92%)]: : 98it [00:19,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.07926000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 95546.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [40960/50000 (82%)]\tLoss: 0.753999: : 49it [00:17,  3.02it/s]\n",
      "notredame Test Epoch: 17 [92160/100000 (92%)]: : 98it [00:18,  5.80it/s]\n",
      "yosemite Test Epoch: 17 [0/100000 (0%)]: : 1it [00:00,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.03020000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 17 [92160/100000 (92%)]: : 98it [00:19,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.08360000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 86960.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 18 [40960/50000 (82%)]\tLoss: 0.755424: : 49it [00:18,  2.65it/s]\n",
      "notredame Test Epoch: 18 [92160/100000 (92%)]: : 98it [00:23,  4.66it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/sets/yosemite.pt\n",
      "\u001b[91mTest set: Accuracy(FPR95): 0.02986000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 18 [92160/100000 (92%)]: : 98it [00:21,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.07540000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n",
      "Generating 50000 triplets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 102865.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19 [40960/50000 (82%)]\tLoss: 0.752878: : 49it [00:17,  2.98it/s]\n",
      "notredame Test Epoch: 19 [92160/100000 (92%)]: : 98it [00:19,  5.70it/s]\n",
      "yosemite Test Epoch: 19 [0/100000 (0%)]: : 1it [00:00,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.02994000\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yosemite Test Epoch: 19 [92160/100000 (92%)]: : 98it [00:19,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTest set: Accuracy(FPR95): 0.07430000\n",
      "\u001b[0m\n",
      "# Found cached data data/sets/liberty.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 100110.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 triplets\n",
      "# Found cached data data/sets/notredame.pt\n",
      "# Found cached data data/sets/yosemite.pt\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "args.epochs = 20\n",
    "start = args.start_epoch\n",
    "end = start + args.epochs\n",
    "try:\n",
    "    logger\n",
    "except NameError:\n",
    "    logger, file_logger = None, None\n",
    "else:\n",
    "    pass\n",
    "triplet_flag = args.load_random_triplets\n",
    "from Losses import loss_DesNet\n",
    "TEST_ON_W1BS = True\n",
    "LOG_DIR = args.log_dir\n",
    "if (args.enable_logging) and (logger == None):\n",
    "    from Loggers import Logger, FileLogger\n",
    "    logger = Logger(LOG_DIR)\n",
    "\n",
    "suffix = '{}_{}_{}'.format(args.experiment_name, args.training_set, args.batch_reduce)\n",
    "if args.gor:\n",
    "    suffix = suffix+'_gor_alpha{:1.1f}'.format(args.alpha)\n",
    "if args.anchorswap:\n",
    "    suffix = suffix + '_as'\n",
    "if args.anchorave:\n",
    "    suffix = suffix + '_av'\n",
    "if args.fliprot:\n",
    "        suffix = suffix + '_fliprot'\n",
    "\n",
    "res_fpr_liberty = torch.zeros(end-start,1)\n",
    "res_fpr_notredame = torch.zeros(end-start, 1)\n",
    "res_fpr_yosemite = torch.zeros(end-start, 1)\n",
    "file = open('results-'+str(CNN)+'.txt', 'w')\n",
    "file.write(\"CNN-\"+str(CNN)+\":\\n[Epoch, Loss, Notredame accuracy, Yosemite accuracy]\\n\")\n",
    "file.close()\n",
    "\n",
    "for epoch in range(start, end):\n",
    "    # iterate over test loaders and test results\n",
    "    train(train_loader, model, optimizer1, epoch, logger, triplet_flag)\n",
    "    for test_loader in test_loaders:\n",
    "        test(test_loader['dataloader'], model, epoch, logger, test_loader['name'])\n",
    "\n",
    "    #randomize train loader batches\n",
    "    train_loader, test_loaders2 = create_loaders(dataset_names, load_random_triplets=triplet_flag)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([5, 200, 1, 32, 32])\n",
      "torch.Size([1000, 1, 32, 32])\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# load patches\n",
    "patches = torch.load(\"../saeedib_keypoints.pt\")\n",
    "print(type(patches))\n",
    "print(patches.shape)\n",
    "patches = patches.view(-1, 1, 32, 32).cuda()\n",
    "print(patches.shape)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Load weights model 1\n",
    "trained_weight_1 = torch.load(\"./models/liberty_train/model-1/checkpoint_19.pth\")\n",
    "model.load_state_dict(trained_weight_1['state_dict'])\n",
    "\n",
    "#Forward pass the patches to the model 1\n",
    "output1 = model(patches)\n",
    "\n",
    "#Resize, output1 result\n",
    "#output.shape = torch.Size([1000, 128])\n",
    "out1 = output1.view(5, 200, 128).cpu().data\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Load weights model 2\n",
    "trained_weight_2 = torch.load(\"./models/liberty_train/model-2/checkpoint_19.pth\")\n",
    "model.load_state_dict(trained_weight_2['state_dict'])\n",
    "\n",
    "#Forward pass the patches to the model 2\n",
    "output2 = model(patches)\n",
    "\n",
    "#Resize, output2 result\n",
    "#output.shape = torch.Size([1000, 128])\n",
    "out2 = output2.view(5, 200, 128).cpu().data\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Load weights model 3\n",
    "trained_weight_3 = torch.load(\"./models/liberty_train/model-3/checkpoint_19.pth\")\n",
    "model.load_state_dict(trained_weight_3['state_dict'])\n",
    "\n",
    "#Forward pass the patches to the model 3\n",
    "output3 = model(patches)\n",
    "\n",
    "#Resize, output3 result\n",
    "#output.shape = torch.Size([1000, 128])\n",
    "out3 = output3.view(5, 200, 128).cpu().data\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load weights model 4\n",
    "trained_weight_4 = torch.load(\"./models/liberty_train/model-4/checkpoint_19.pth\")\n",
    "model.load_state_dict(trained_weight_4['state_dict'])\n",
    "\n",
    "#Forward pass the patches to the model 4\n",
    "output3 = model(patches)\n",
    "\n",
    "#Resize, output3 result\n",
    "#output.shape = torch.Size([1000, 128])\n",
    "out3 = output3.view(5, 200, 128).cpu().data\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-86c1af0b28d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Store themtogether:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../Saeedib_descriptions.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Torch' is not defined"
     ]
    }
   ],
   "source": [
    "#Store themtogether:\n",
    "all_output = torch.stack((out1, out2, out3))\n",
    "print(all_output.shape)\n",
    "output_dir = \"../Saeedib_descriptions.pt\"\n",
    "torch.save(all_output, output_dir)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
